{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Preprocessing 2: Feature Engineering cho 2017 Movies\n",
    "## Kết hợp phiên bản cũ và mới với các kỹ thuật tiền xử lý nâng cao\n",
    "\n",
    "### Mục tiêu:\n",
    "- Xử lý dữ liệu phim 2017\n",
    "- Feature Engineering nâng cao\n",
    "- Data Validation và Cleaning\n",
    "- JSON Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict\n",
    "import logging\n",
    "\n",
    "# Cấu hình logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_json_data(df: pd.DataFrame, json_columns: List[str]) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate tính toàn vẹn của dữ liệu JSON và trả về báo cáo chi tiết\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame cần validate\n",
    "        json_columns: List các cột chứa dữ liệu JSON\n",
    "        \n",
    "    Returns:\n",
    "        Dict chứa thống kê validation cho mỗi cột\n",
    "    \"\"\"\n",
    "    validation_report = {}\n",
    "    logging.info(\"=== JSON DATA VALIDATION ===\")\n",
    "    \n",
    "    for col in json_columns:\n",
    "        if col in df.columns:\n",
    "            stats = {\n",
    "                'valid_json': 0,\n",
    "                'invalid_json': 0,\n",
    "                'empty_json': 0,\n",
    "                'total_rows': len(df)\n",
    "            }\n",
    "            \n",
    "            for idx, value in df[col].items():\n",
    "                if pd.isna(value) or value == '[]' or value == '':\n",
    "                    stats['empty_json'] += 1\n",
    "                else:\n",
    "                    try:\n",
    "                        parsed = ast.literal_eval(value)\n",
    "                        if isinstance(parsed, list):\n",
    "                            stats['valid_json'] += 1\n",
    "                        else:\n",
    "                            stats['invalid_json'] += 1\n",
    "                    except (ValueError, SyntaxError):\n",
    "                        stats['invalid_json'] += 1\n",
    "            \n",
    "            stats['validity_rate'] = (stats['valid_json'] / len(df) * 100)\n",
    "            validation_report[col] = stats\n",
    "            \n",
    "            logging.info(f\"\\n{col}:\")\n",
    "            logging.info(f\"  Valid JSON: {stats['valid_json']}\")\n",
    "            logging.info(f\"  Invalid JSON: {stats['invalid_json']}\")\n",
    "            logging.info(f\"  Empty JSON: {stats['empty_json']}\")\n",
    "            logging.info(f\"  Validity rate: {stats['validity_rate']:.2f}%\")\n",
    "    \n",
    "    return validation_report\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Load và khám phá dữ liệu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load và xử lý dữ liệu ban đầu\n",
    "credits = pd.read_csv('../datasets/credits.csv')\n",
    "meta = pd.read_csv('../datasets/movie_metadata.csv')\n",
    "\n",
    "logging.info(\"=== DATASET INFORMATION ===\")\n",
    "logging.info(f\"Credits shape: {credits.shape}\")\n",
    "logging.info(f\"Metadata shape: {meta.shape}\")\n",
    "\n",
    "# Validate JSON columns trong credits dataset\n",
    "json_columns = ['cast', 'crew', 'genres']\n",
    "validation_results = validate_json_data(credits, json_columns)\n",
    "\n",
    "# Xử lý ngày tháng và lọc dữ liệu 2017\n",
    "meta['release_date'] = pd.to_datetime(meta['release_date'], errors='coerce')\n",
    "meta['year'] = meta['release_date'].dt.year\n",
    "\n",
    "# Hiển thị phân bố theo năm\n",
    "logging.info(\"\\n=== YEAR DISTRIBUTION ===\")\n",
    "year_dist = meta['year'].value_counts().sort_index()\n",
    "display(year_dist)\n",
    "\n",
    "# Lọc phim 2017\n",
    "new_meta = meta.loc[meta.year == 2017, ['genres', 'id', 'title', 'year']]\n",
    "new_meta['id'] = new_meta['id'].astype(int)\n",
    "\n",
    "# Merge dữ liệu\n",
    "data = pd.merge(new_meta, credits, on='id')\n",
    "pd.set_option('display.max_colwidth', 75)\n",
    "\n",
    "logging.info(\"\\n=== MERGED DATA INFORMATION ===\")\n",
    "logging.info(f\"Shape after merge: {data.shape}\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Quality Report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_data_quality_report(df: pd.DataFrame, dataset_name: str = \"Dataset\") -> Dict:\n",
    "    \"\"\"\n",
    "    Tạo báo cáo chất lượng dữ liệu chi tiết với các chỉ số bổ sung\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame cần phân tích\n",
    "        dataset_name: Tên của dataset để hiển thị trong báo cáo\n",
    "    \n",
    "    Returns:\n",
    "        Dict chứa các chỉ số chất lượng dữ liệu\n",
    "    \"\"\"\n",
    "    quality_metrics = {}\n",
    "    \n",
    "    logging.info(f\"\\n=== {dataset_name.upper()} QUALITY REPORT ===\")\n",
    "    logging.info(f\"Shape: {df.shape}\")\n",
    "    \n",
    "    # 1. Missing values analysis\n",
    "    missing_data = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Missing_Count': df.isnull().sum(),\n",
    "        'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "        'Data_Type': df.dtypes\n",
    "    })\n",
    "    missing_data = missing_data.sort_values('Missing_Percentage', ascending=False)\n",
    "    quality_metrics['missing_data'] = missing_data\n",
    "    \n",
    "    # 2. Duplicate analysis\n",
    "    duplicates = df.duplicated().sum()\n",
    "    duplicate_rate = (duplicates/len(df)*100)\n",
    "    quality_metrics['duplicates'] = {\n",
    "        'count': duplicates,\n",
    "        'percentage': duplicate_rate\n",
    "    }\n",
    "    \n",
    "    # 3. Data type distribution\n",
    "    dtype_dist = df.dtypes.value_counts()\n",
    "    quality_metrics['dtype_distribution'] = dtype_dist\n",
    "    \n",
    "    # 4. Unique values analysis\n",
    "    unique_counts = {}\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in ['object', 'string', 'category']:\n",
    "            unique_counts[col] = df[col].nunique()\n",
    "    quality_metrics['unique_values'] = unique_counts\n",
    "    \n",
    "    # Display results\n",
    "    logging.info(\"\\n=== MISSING VALUES ANALYSIS ===\")\n",
    "    display(missing_data[missing_data['Missing_Count'] > 0])\n",
    "    \n",
    "    logging.info(f\"\\n=== DUPLICATE ANALYSIS ===\")\n",
    "    logging.info(f\"Total duplicates: {duplicates} ({duplicate_rate:.2f}%)\")\n",
    "    \n",
    "    logging.info(\"\\n=== DATA TYPE DISTRIBUTION ===\")\n",
    "    display(dtype_dist)\n",
    "    \n",
    "    logging.info(\"\\n=== UNIQUE VALUES IN CATEGORICAL COLUMNS ===\")\n",
    "    for col, count in unique_counts.items():\n",
    "        logging.info(f\"{col}: {count} unique values\")\n",
    "    \n",
    "    return quality_metrics\n",
    "\n",
    "# Chạy quality report cho cả 3 dataset\n",
    "data_quality = enhanced_data_quality_report(data, \"Movies 2017\")\n",
    "credits_quality = enhanced_data_quality_report(credits, \"Credits\")\n",
    "meta_quality = enhanced_data_quality_report(meta, \"Metadata\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. JSON Data Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_json_parse(json_str: str, default_value: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse JSON string an toàn với logging và validation\n",
    "    \n",
    "    Args:\n",
    "        json_str: Chuỗi JSON cần parse\n",
    "        default_value: Giá trị mặc định nếu parse thất bại (default: {})\n",
    "    \n",
    "    Returns:\n",
    "        Dict đã parse hoặc default value\n",
    "    \"\"\"\n",
    "    if default_value is None:\n",
    "        default_value = {}\n",
    "        \n",
    "    try:\n",
    "        if pd.isna(json_str):\n",
    "            return default_value\n",
    "        parsed_data = ast.literal_eval(json_str)\n",
    "        if not isinstance(parsed_data, list):\n",
    "            logging.warning(f\"JSON data không phải là list: {json_str[:100]}...\")\n",
    "            return default_value\n",
    "        return parsed_data\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Lỗi khi parse JSON: {str(e)[:100]}...\")\n",
    "        return default_value\n",
    "\n",
    "def process_json_columns(df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Xử lý nhiều cột JSON với validation và logging\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame cần xử lý\n",
    "        columns: List các cột JSON cần xử lý\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame đã xử lý\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            logging.info(f\"Processing {col} column...\")\n",
    "            df[col] = df[col].apply(safe_json_parse)\n",
    "            \n",
    "            # Validate kết quả\n",
    "            valid_rows = df[col].apply(lambda x: isinstance(x, list)).sum()\n",
    "            total_rows = len(df)\n",
    "            success_rate = (valid_rows / total_rows) * 100\n",
    "            \n",
    "            logging.info(f\"{col} processing results:\")\n",
    "            logging.info(f\"- Valid rows: {valid_rows}/{total_rows}\")\n",
    "            logging.info(f\"- Success rate: {success_rate:.2f}%\")\n",
    "            \n",
    "            # Sample data\n",
    "            if len(df) > 0:\n",
    "                logging.info(f\"\\nSample {col} data:\")\n",
    "                logging.info(df[col].iloc[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Xử lý JSON columns cho cả hai dataset\n",
    "json_columns = ['cast', 'crew', 'genres']\n",
    "data = process_json_columns(data, json_columns)\n",
    "credits = process_json_columns(credits, json_columns)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xử lý genres\n",
    "def make_genresList(x):\n",
    "    gen = []\n",
    "    st = \" \"\n",
    "    for i in x:\n",
    "        if i.get('name') == 'Science Fiction':\n",
    "            scifi = 'Sci-Fi'\n",
    "            gen.append(scifi)\n",
    "        else:\n",
    "            gen.append(i.get('name'))\n",
    "    if gen == []:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return (st.join(gen))\n",
    "\n",
    "# Xử lý actors\n",
    "def get_actor1(x):\n",
    "    casts = []\n",
    "    for i in x:\n",
    "        casts.append(i.get('name'))\n",
    "    if casts == []:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return (casts[0])\n",
    "\n",
    "def get_actor2(x):\n",
    "    casts = []\n",
    "    for i in x:\n",
    "        casts.append(i.get('name'))\n",
    "    if casts == [] or len(casts)<=1:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return (casts[1])\n",
    "\n",
    "def get_actor3(x):\n",
    "    casts = []\n",
    "    for i in x:\n",
    "        casts.append(i.get('name'))\n",
    "    if casts == [] or len(casts)<=2:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return (casts[2])\n",
    "\n",
    "# Xử lý directors\n",
    "def get_directors(x):\n",
    "    dt = []\n",
    "    st = \" \"\n",
    "    for i in x:\n",
    "        if i.get('job') == 'Director':\n",
    "            dt.append(i.get('name'))\n",
    "    if dt == []:\n",
    "        return np.NaN\n",
    "    else:\n",
    "        return (st.join(dt))\n",
    "\n",
    "# Áp dụng các hàm xử lý\n",
    "data['genres_list'] = data['genres'].map(lambda x: make_genresList(x))\n",
    "data['actor_1_name'] = data['cast'].map(lambda x: get_actor1(x))\n",
    "data['actor_2_name'] = data['cast'].map(lambda x: get_actor2(x))\n",
    "data['actor_3_name'] = data['cast'].map(lambda x: get_actor3(x))\n",
    "data['director_name'] = data['crew'].map(lambda x: get_directors(x))\n",
    "\n",
    "# Tạo DataFrame movie với các cột cần thiết\n",
    "movie = data.loc[:, ['director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'genres_list', 'title']]\n",
    "\n",
    "# Kiểm tra missing values\n",
    "logging.info(\"\\n=== MISSING VALUES CHECK ===\")\n",
    "display(movie.isna().sum())\n",
    "\n",
    "# Xóa các dòng có missing values\n",
    "movie = movie.dropna(how='any')\n",
    "logging.info(\"\\n=== AFTER DROPPING NA ===\")\n",
    "display(movie.isna().sum())\n",
    "\n",
    "# Đổi tên cột\n",
    "movie = movie.rename(columns={'genres_list': 'genres', 'title': 'movie_title'})\n",
    "movie['movie_title'] = movie['movie_title'].str.lower()\n",
    "\n",
    "# Tạo cột combined features\n",
    "movie['comb'] = movie['actor_1_name'] + ' ' + movie['actor_2_name'] + ' ' + movie['actor_3_name'] + ' ' + movie['director_name'] + ' ' + movie['genres']\n",
    "\n",
    "logging.info(\"\\n=== FINAL PROCESSED DATA ===\")\n",
    "display(movie.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\\n=== TF-IDF PROCESSING ===\")\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "genres_tfidf = tfidf.fit_transform(movie['genres'])  \n",
    "genres_df = pd.DataFrame(genres_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "movie = pd.concat([movie, genres_df], axis=1)  \n",
    "\n",
    "print(\"Sample TF-IDF features:\")\n",
    "display(genres_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dữ liệu cũ để kết hợp\n",
    "try:\n",
    "    old = pd.read_csv('../datasets/data.csv')\n",
    "    logging.info(\"\\n=== LOADING OLD DATA ===\")\n",
    "    logging.info(f\"Old data shape: {old.shape}\")\n",
    "    \n",
    "    # Tạo cột combined features cho dữ liệu cũ\n",
    "    old['comb'] = old['actor_1_name'] + ' ' + old['actor_2_name'] + ' ' + old['actor_3_name'] + ' ' + old['director_name'] + ' ' + old['genres']\n",
    "    \n",
    "    # Kết hợp dữ liệu cũ và mới\n",
    "    final_data = pd.concat([old, movie], ignore_index=True)\n",
    "    logging.info(f\"\\nCombined data shape: {final_data.shape}\")\n",
    "\n",
    "    final_data = final_data.drop_duplicates(subset=\"movie_title\", keep='last')\n",
    "    logging.info(f\"After removing duplicates: {final_data.shape}\")\n",
    "    \n",
    "    # Xuất ra file\n",
    "    output_file = '../datasets/new_data.csv'\n",
    "    final_data.to_csv(output_file, index=False)\n",
    "    logging.info(f\"\\nĐã xuất dữ liệu kết hợp ra file: {output_file}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logging.warning(\"\\nKhông tìm thấy file data.csv cũ, chỉ xuất dữ liệu mới\")\n",
    "\n",
    "    processed_movie_data = movie.drop_duplicates(subset=\"movie_title\", keep='last')\n",
    "    logging.info(f\"After removing duplicates: {processed_movie_data.shape}\")\n",
    "    \n",
    "    output_file = '../datasets/new_data.csv'\n",
    "    processed_movie_data.to_csv(output_file, index=False) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Text Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 6. Export Processed Data\n",
    "\n",
    "# Lọc và chuẩn bị dữ liệu cuối cùng\n",
    "final_columns = [\n",
    "    'movie_title', 'director_name', \n",
    "    'actor_1_name', 'actor_2_name', 'actor_3_name',\n",
    "    'genres'\n",
    "] + list(genres_df.columns)  # Thêm các cột TF-IDF\n",
    "\n",
    "# Tạo DataFrame cuối cùng với các cột đã chọn\n",
    "final_data = data[final_columns].copy()\n",
    "\n",
    "# Kiểm tra dữ liệu trước khi xuất\n",
    "logging.info(\"\\n=== FINAL DATA QUALITY CHECK ===\")\n",
    "logging.info(f\"Shape: {final_data.shape}\")\n",
    "logging.info(f\"Memory usage: {final_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "logging.info(\"\\nSample of final data:\")\n",
    "display(final_data.head())\n",
    "\n",
    "# Xuất ra file CSV\n",
    "output_file = '../datasets/movies_2017_processed.csv'\n",
    "final_data.to_csv(output_file, index=False)\n",
    "logging.info(f\"\\nĐã xuất dữ liệu đã xử lý ra file: {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
