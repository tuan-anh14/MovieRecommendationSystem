{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Preprocessing 4: 2020 Movies Data Integration\n",
    "## Enhanced version with improved validation and processing\n",
    "\n",
    "### Goals:\n",
    "1. Extract 2020 movies data from Wikipedia\n",
    "2. Add genre information from TMDB API\n",
    "3. Process director and actor information\n",
    "4. Validate and clean data\n",
    "5. Merge with existing dataset\n",
    "6. Add quality checks and logging\n",
    "\n",
    "### Improvements:\n",
    "- Added data validation and error handling\n",
    "- Enhanced logging and progress tracking\n",
    "- Improved code structure and documentation\n",
    "- Added data quality checks\n",
    "- Optimized data processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "from tmdbv3api import TMDb, Movie\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Union\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure TMDB\n",
    "tmdb = TMDb()\n",
    "tmdb.api_key = ''  # Add your API key here\n",
    "tmdb_movie = Movie()\n",
    "\n",
    "# Helper functions for data validation\n",
    "def validate_string(s: str) -> bool:\n",
    "    \"\"\"Validate if a string is non-empty and contains valid characters.\"\"\"\n",
    "    return bool(s and isinstance(s, str) and not s.isspace())\n",
    "\n",
    "def validate_list(lst: list) -> bool:\n",
    "    \"\"\"Validate if a list is non-empty and contains valid elements.\"\"\"\n",
    "    return bool(lst and isinstance(lst, list) and all(validate_string(x) for x in lst))\n",
    "\n",
    "def safe_request(url: str) -> Optional[str]:\n",
    "    \"\"\"Make a safe HTTP request with error handling.\"\"\"\n",
    "    try:\n",
    "        return urllib.request.urlopen(url).read()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error fetching URL {url}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction functions\n",
    "def get_genre(title: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Get movie genres from TMDB API with error handling and validation.\n",
    "    \n",
    "    Args:\n",
    "        title: Movie title to search for\n",
    "        \n",
    "    Returns:\n",
    "        String of genres separated by spaces or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = tmdb_movie.search(title)\n",
    "        if not result:\n",
    "            logging.warning(f\"No TMDB results found for movie: {title}\")\n",
    "            return None\n",
    "            \n",
    "        movie_id = result[0].id\n",
    "        response = requests.get(f'https://api.themoviedb.org/3/movie/{movie_id}?api_key={tmdb.api_key}')\n",
    "        data = response.json()\n",
    "        \n",
    "        if not data.get('genres'):\n",
    "            logging.warning(f\"No genres found for movie: {title}\")\n",
    "            return None\n",
    "            \n",
    "        genres = [genre['name'] for genre in data['genres']]\n",
    "        return \" \".join(genres)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error getting genres for {title}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def extract_name(text: str, role: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract name from text based on role with validation.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract name from\n",
    "        role: Role to look for (director/actor)\n",
    "        \n",
    "    Returns:\n",
    "        Extracted name or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if f\" ({role})\" in text:\n",
    "            name = text.split(f\" ({role})\")[0]\n",
    "            return name if validate_string(name) else None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting {role} name: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def get_director(text: str) -> Optional[str]:\n",
    "    \"\"\"Extract director name with validation.\"\"\"\n",
    "    for role in [\"director\", \"directors\", \"director/screenplay\"]:\n",
    "        name = extract_name(text, role)\n",
    "        if name:\n",
    "            return name\n",
    "    return None\n",
    "\n",
    "def get_actor(text: str, position: int) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract actor name for given position with validation.\n",
    "    \n",
    "    Args:\n",
    "        text: Text containing actor information\n",
    "        position: Position of actor (1-3)\n",
    "        \n",
    "    Returns:\n",
    "        Actor name or None if not found\n",
    "    \"\"\"\n",
    "    try:\n",
    "        actors = (text.split(\"screenplay); \")[-1]).split(\", \")\n",
    "        if len(actors) >= position:\n",
    "            name = actors[position-1]\n",
    "            return name if validate_string(name) else None\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error extracting actor {position} name: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data extraction from Wikipedia\n",
    "logging.info(\"Starting Wikipedia data extraction...\")\n",
    "\n",
    "# Wikipedia URL for 2020 movies\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_American_films_of_2020\"\n",
    "\n",
    "# Get page content\n",
    "source = safe_request(WIKI_URL)\n",
    "if not source:\n",
    "    raise RuntimeError(\"Failed to fetch Wikipedia page\")\n",
    "\n",
    "# Parse HTML\n",
    "soup = bs.BeautifulSoup(source, 'lxml')\n",
    "tables = soup.find_all('table', class_='wikitable sortable')\n",
    "\n",
    "if not tables:\n",
    "    raise RuntimeError(\"No tables found on Wikipedia page\")\n",
    "\n",
    "logging.info(f\"Found {len(tables)} quarterly tables\")\n",
    "\n",
    "# Extract data from all tables\n",
    "dfs = []\n",
    "for i, table in enumerate(tables, 1):\n",
    "    try:\n",
    "        df = pd.read_html(str(table))[0]\n",
    "        logging.info(f\"Extracted {len(df)} movies from Q{i} table\")\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing Q{i} table: {str(e)}\")\n",
    "\n",
    "# Combine all tables\n",
    "df_2020 = pd.concat(dfs, ignore_index=True)\n",
    "logging.info(f\"Total movies extracted: {len(df_2020)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of extracted data:\")\n",
    "display(df_2020.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and clean the data\n",
    "logging.info(\"Starting data processing...\")\n",
    "\n",
    "# Select required columns\n",
    "df_2020 = df_2020[['Title', 'Cast and crew']]\n",
    "\n",
    "# Add genres using TMDB API\n",
    "logging.info(\"Fetching genres from TMDB API...\")\n",
    "df_2020['genres'] = df_2020['Title'].progress_apply(get_genre)\n",
    "\n",
    "# Extract director and actors\n",
    "logging.info(\"Extracting director and actor information...\")\n",
    "df_2020['director_name'] = df_2020['Cast and crew'].apply(get_director)\n",
    "df_2020['actor_1_name'] = df_2020['Cast and crew'].apply(lambda x: get_actor(x, 1))\n",
    "df_2020['actor_2_name'] = df_2020['Cast and crew'].apply(lambda x: get_actor(x, 2))\n",
    "df_2020['actor_3_name'] = df_2020['Cast and crew'].apply(lambda x: get_actor(x, 3))\n",
    "\n",
    "# Rename title column\n",
    "df_2020 = df_2020.rename(columns={'Title': 'movie_title'})\n",
    "\n",
    "# Convert movie titles to lowercase\n",
    "df_2020['movie_title'] = df_2020['movie_title'].str.lower()\n",
    "\n",
    "# Create combined features column\n",
    "df_2020['comb'] = (df_2020['actor_1_name'] + ' ' + \n",
    "                   df_2020['actor_2_name'] + ' ' + \n",
    "                   df_2020['actor_3_name'] + ' ' + \n",
    "                   df_2020['director_name'] + ' ' + \n",
    "                   df_2020['genres'])\n",
    "\n",
    "# Select final columns\n",
    "final_cols = ['director_name', 'actor_1_name', 'actor_2_name', \n",
    "              'actor_3_name', 'genres', 'movie_title', 'comb']\n",
    "new_df20 = df_2020[final_cols]\n",
    "\n",
    "# Data quality checks\n",
    "logging.info(\"\\nData quality check:\")\n",
    "logging.info(f\"Total rows: {len(new_df20)}\")\n",
    "logging.info(\"\\nMissing values:\")\n",
    "display(new_df20.isna().sum())\n",
    "\n",
    "# Drop rows with missing values\n",
    "new_df20 = new_df20.dropna(how='any')\n",
    "logging.info(f\"\\nRows after dropping missing values: {len(new_df20)}\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nSample of processed data:\")\n",
    "display(new_df20.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with existing dataset\n",
    "logging.info(\"Starting dataset merge...\")\n",
    "\n",
    "try:\n",
    "    # Load existing dataset\n",
    "    old_df = pd.read_csv('final_data.csv')\n",
    "    logging.info(f\"Loaded existing dataset with {len(old_df)} rows\")\n",
    "    \n",
    "    # Check for duplicate movies\n",
    "    duplicates = pd.concat([old_df['movie_title'], new_df20['movie_title']]).duplicated()\n",
    "    duplicate_count = sum(duplicates)\n",
    "    if duplicate_count > 0:\n",
    "        logging.warning(f\"Found {duplicate_count} duplicate movies\")\n",
    "    \n",
    "    # Merge datasets\n",
    "    final_df = pd.concat([old_df, new_df20], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates if any\n",
    "    final_df = final_df.drop_duplicates(subset=['movie_title'], keep='first')\n",
    "    \n",
    "    logging.info(f\"Final dataset shape: {final_df.shape}\")\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(\"\\nFinal dataset summary:\")\n",
    "    print(f\"Total movies: {len(final_df)}\")\n",
    "    print(f\"Unique directors: {final_df['director_name'].nunique()}\")\n",
    "    print(f\"Unique actors: {len(set(final_df['actor_1_name'].tolist() + final_df['actor_2_name'].tolist() + final_df['actor_3_name'].tolist()))}\")\n",
    "    print(f\"Unique genres: {len(set(' '.join(final_df['genres'].fillna('')).split()))}\")\n",
    "    \n",
    "    # Save final dataset\n",
    "    final_df.to_csv('main_data.csv', index=False)\n",
    "    logging.info(\"Successfully saved final dataset to main_data.csv\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    logging.warning(\"No existing dataset found, saving only 2020 data\")\n",
    "    new_df20.to_csv('main_data.csv', index=False)\n",
    "    logging.info(\"Successfully saved 2020 dataset to main_data.csv\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error during dataset merge: {str(e)}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Preprocessing 4: Final Data Integration và Optimization\n",
    "## Kết hợp phiên bản cũ và mới với các kỹ thuật tiền xử lý nâng cao\n",
    "\n",
    "### Mục tiêu:\n",
    "- Final Data Integration\n",
    "- Feature Engineering nâng cao\n",
    "- Data Optimization\n",
    "- Quality Assurance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import các thư viện cần thiết\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 1. Load dữ liệu đã xử lý\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dữ liệu đã xử lý\n",
    "data_old = pd.read_csv('data.csv')\n",
    "data_2017 = pd.read_csv('data_2017_integrated.csv')\n",
    "\n",
    "print(f\"Old Dataset shape: {data_old.shape}\")\n",
    "print(f\"2017 Dataset shape: {data_2017.shape}\")\n",
    "\n",
    "# Hiển thị sample data\n",
    "print(\"\\nSample old data:\")\n",
    "display(data_old.head(3))\n",
    "print(\"\\nSample 2017 data:\")\n",
    "display(data_2017.head(3))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 2. Data Integration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine datasets\n",
    "data = pd.concat([data_old, data_2017], ignore_index=True)\n",
    "print(f\"Combined Dataset shape: {data.shape}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = data.duplicated(subset=['movie_title']).sum()\n",
    "print(f\"\\nDuplicates found: {duplicates}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates > 0:\n",
    "    data = data.drop_duplicates(subset=['movie_title'], keep='first')\n",
    "    print(f\"Shape after removing duplicates: {data.shape}\")\n",
    "\n",
    "# Display sample of combined data\n",
    "print(\"\\nSample combined data:\")\n",
    "display(data.head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 3. Advanced Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization cho genres\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "genres_tfidf = tfidf.fit_transform(data['genres'])\n",
    "genres_df = pd.DataFrame(genres_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "data = pd.concat([data, genres_df], axis=1)\n",
    "\n",
    "# Label Encoding cho categorical variables\n",
    "le = LabelEncoder()\n",
    "data['director_encoded'] = le.fit_transform(data['director_name'])\n",
    "data['actor1_encoded'] = le.fit_transform(data['actor_1_name'])\n",
    "data['actor2_encoded'] = le.fit_transform(data['actor_2_name'])\n",
    "data['actor3_encoded'] = le.fit_transform(data['actor_3_name'])\n",
    "\n",
    "print(\"Features after engineering:\")\n",
    "print(\"\\nGenre features:\", genres_df.columns.tolist())\n",
    "print(\"\\nEncoded features:\", ['director_encoded', 'actor1_encoded', 'actor2_encoded', 'actor3_encoded'])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 4. Data Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_memory(df):\n",
    "    \"\"\"\n",
    "    Optimize memory usage của DataFrame\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Optimize memory\n",
    "print(\"Memory usage before optimization:\", data.memory_usage(deep=True).sum() / 1024**2, \"MB\")\n",
    "data = optimize_memory(data)\n",
    "print(\"Memory usage after optimization:\", data.memory_usage(deep=True).sum() / 1024**2, \"MB\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 5. Quality Assurance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_assurance(df):\n",
    "    \"\"\"\n",
    "    Kiểm tra chất lượng dữ liệu cuối cùng\n",
    "    \"\"\"\n",
    "    print(\"=== FINAL QUALITY ASSURANCE ===\")\n",
    "    \n",
    "    # Check missing values\n",
    "    missing = df.isnull().sum()\n",
    "    print(\"\\nMissing values:\")\n",
    "    print(missing[missing > 0] if missing.any() else \"No missing values\")\n",
    "    \n",
    "    # Check duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicates: {duplicates} ({(duplicates/len(df)*100):.2f}%)\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    # Check memory usage\n",
    "    print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Check feature distributions\n",
    "    print(\"\\nFeature statistics:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    display(df[numeric_cols].describe())\n",
    "    \n",
    "    # Check text data quality\n",
    "    print(\"\\nText data quality:\")\n",
    "    text_cols = ['movie_title', 'director_name', 'actor_1_name', 'actor_2_name', 'actor_3_name', 'genres']\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            empty = df[df[col].str.len() == 0].shape[0] if df[col].dtype == object else 0\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  - Empty strings: {empty}\")\n",
    "            print(f\"  - Unique values: {df[col].nunique()}\")\n",
    "            print(f\"  - Sample values: {df[col].sample(3).tolist()}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "quality_assurance(data)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 6. Visualization của Feature Distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numeric feature distributions\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns[:6]  # Lấy 6 cột đầu tiên để demo\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(numeric_cols):\n",
    "    sns.histplot(data=data, x=col, ax=axes[idx])\n",
    "    axes[idx].set_title(f'Distribution of {col}')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(data[numeric_cols].corr(), annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlations')\n",
    "plt.show()\n",
    "\n",
    "# Top directors và actors\n",
    "plt.figure(figsize=(12, 6))\n",
    "data['director_name'].value_counts().head(10).plot(kind='barh')\n",
    "plt.title('Top 10 Directors')\n",
    "plt.xlabel('Number of Movies')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "data['actor_1_name'].value_counts().head(10).plot(kind='barh')\n",
    "plt.title('Top 10 Lead Actors')\n",
    "plt.xlabel('Number of Movies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## 7. Lưu dữ liệu cuối cùng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu dữ liệu đã xử lý\n",
    "data.to_csv('main_data.csv', index=False)\n",
    "print(\"Final data saved successfully!\")\n",
    "\n",
    "# Hiển thị thông tin cuối cùng\n",
    "print(\"\\nFinal dataset information:\")\n",
    "print(f\"Shape: {data.shape}\")\n",
    "print(f\"Memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Number of features: {len(data.columns)}\")\n",
    "print(\"\\nFeature types:\")\n",
    "print(data.dtypes.value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
